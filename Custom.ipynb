{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium==0.29.1\n",
    "%pip install rl-zoo3==2.2.1\n",
    "%pip install sb3-contrib==2.2.1\n",
    "%pip install stable-baselines3==2.2.1\n",
    "%pip install panda-gym==3.0.7\n",
    "\n",
    "# %pip install torch==2.2.0\n",
    "# python 310\n",
    "\n",
    "# verbose = 0  (출력하지 않음 X)\n",
    "# verbose = 1  (정보를 상세하게 출력함)\n",
    "# verbose = 2  (정보를 함축적으로 출력함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panda_gym\n",
    "import sys\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "from sb3_contrib import TQC\n",
    "from stable_baselines3.common.logger import configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `초기화용`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/sb3_log/\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# gymnasium을 gym으로 인식시키기\n",
    "sys.modules[\"gym\"] = gymnasium\n",
    "\n",
    "# 로거 설정\n",
    "tmp_path = \"/tmp/sb3_log/\"\n",
    "new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "# 환경 생성\n",
    "env = gymnasium.make(\"PandaPickAndPlace-v3\")\n",
    "\n",
    "# TQC 모델 초기화\n",
    "# model = TQC(policy=\"MultiInputPolicy\", env=env, verbose=1)\n",
    "model = TQC.load(\"ppap_updated\", env=env)\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# 초기 경험 수집\n",
    "initial_steps = 1000  # 필요한 초기 단계 수\n",
    "obs, _ = env.reset()\n",
    "for _ in range(initial_steps):\n",
    "    action = env.action_space.sample()  # 랜덤 행동\n",
    "    next_obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # 관찰 값이 리스트나 튜플일 경우 처리\n",
    "    if isinstance(obs, (list, tuple)):\n",
    "        obs = np.array(obs)\n",
    "        next_obs = np.array(next_obs)\n",
    "\n",
    "    # 관찰 값이 딕셔너리가 아닌 경우 처리 및 추가 키 포함\n",
    "    if not isinstance(obs, dict):\n",
    "        obs_dict = {\n",
    "            'observation': obs.flatten(),\n",
    "            'achieved_goal': obs.flatten()[:3],  # 필요에 맞게 수정\n",
    "            'desired_goal': obs.flatten()[3:6]   # 필요에 맞게 수정\n",
    "        }\n",
    "        next_obs_dict = {\n",
    "            'observation': next_obs.flatten(),\n",
    "            'achieved_goal': next_obs.flatten()[:3],  # 필요에 맞게 수정\n",
    "            'desired_goal': next_obs.flatten()[3:6]   # 필요에 맞게 수정\n",
    "        }\n",
    "    else:\n",
    "        obs_dict = obs\n",
    "        next_obs_dict = next_obs\n",
    "    \n",
    "    model.replay_buffer.add(obs_dict, next_obs_dict, action, reward, done, [info])\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "    else:\n",
    "        obs = next_obs\n",
    "\n",
    "# 모델 학습\n",
    "model.train(gradient_steps=5000)\n",
    "\n",
    "# 모델 저장\n",
    "model.save(\"ppap_updated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `업데이트 버전`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/sb3_log/\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "high <= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mset_logger(new_logger)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppap_updated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\miniconda3\\envs\\gymenv\\Lib\\site-packages\\sb3_contrib\\tqc\\tqc.py:209\u001b[0m, in \u001b[0;36mTQC.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    205\u001b[0m actor_losses, critic_losses \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gradient_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(gradient_steps):\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Sample replay buffer\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     replay_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vec_normalize_env\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# We need to sample because `log_std` may have changed between two gradient steps\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde:\n",
      "File \u001b[1;32me:\\miniconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:660\u001b[0m, in \u001b[0;36mDictReplayBuffer.sample\u001b[1;34m(self, batch_size, env)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    649\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    650\u001b[0m     env: Optional[VecNormalize] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    651\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DictReplayBufferSamples:\n\u001b[0;32m    652\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;124;03m    Sample elements from the replay buffer.\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mReplayBuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\miniconda3\\envs\\gymenv\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:113\u001b[0m, in \u001b[0;36mBaseBuffer.sample\u001b[1;34m(self, batch_size, env)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m:param batch_size: Number of element to sample\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m:param env: associated gym VecEnv\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    to normalize the observations/rewards when sampling\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    112\u001b[0m upper_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos\n\u001b[1;32m--> 113\u001b[0m batch_inds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, upper_bound, size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_samples(batch_inds, env\u001b[38;5;241m=\u001b[39menv)\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:780\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mnumpy\\\\random\\\\_bounded_integers.pyx:2885\u001b[0m, in \u001b[0;36mnumpy.random._bounded_integers._rand_int32\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: high <= 0"
     ]
    }
   ],
   "source": [
    "# gymnasium을 gym으로 인식시키기\n",
    "sys.modules[\"gym\"] = gymnasium\n",
    "\n",
    "# 로거 설정\n",
    "tmp_path = \"/tmp/sb3_log/\"\n",
    "new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "# 환경 생성\n",
    "env = gymnasium.make(\"PandaPickAndPlace-v3\")\n",
    "\n",
    "# 저장된 모델 불러오기\n",
    "model = TQC.load(\"ppap_updated\", env=env)\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# 모델 학습\n",
    "model.train(gradient_steps=10000)\n",
    "\n",
    "# 모델 저장\n",
    "model.save(\"ppap_updated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from numpngw import write_apng\n",
    "# gymnasium을 gym으로 인식시키기\n",
    "# sys.modules[\"gym\"] = gymnasium\n",
    "\n",
    "# # 로거 설정\n",
    "# tmp_path = \"/tmp/sb3_log/\"\n",
    "# new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "# 환경 생성\n",
    "# env = gymnasium.make(\"PandaPickAndPlace-v3\")\n",
    "\n",
    "# 저장된 모델 불러오기\n",
    "model = TQC.load(\"ppap_updated\", env=env)\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# 이미지 리스트 초기화\n",
    "images = []\n",
    "\n",
    "# 환경 초기화 및 모델 평가\n",
    "obs, info = env.reset()\n",
    "for _ in range(100):  # 1000 스텝 동안 평가\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # 각 스텝에서 이미지를 캡처하여 리스트에 추가\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "    \n",
    "    if done or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "\n",
    "# 이미지 리스트를 APNG로 저장\n",
    "write_apng(\"testAnim.png\", images, delay=50)  # 200 ms 딜레이로 애니메이션 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v3 as iio\n",
    "iio.imwrite(\"anim.mp4\", images, fps=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/sb3_log/\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "high <= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m TQC(policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m=\u001b[39menv, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mset_logger(new_logger)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppap\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Languages\\anaconda3\\envs\\gym310\\lib\\site-packages\\sb3_contrib\\tqc\\tqc.py:209\u001b[0m, in \u001b[0;36mTQC.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    205\u001b[0m actor_losses, critic_losses \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gradient_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(gradient_steps):\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Sample replay buffer\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     replay_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vec_normalize_env\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# We need to sample because `log_std` may have changed between two gradient steps\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde:\n",
      "File \u001b[1;32me:\\Languages\\anaconda3\\envs\\gym310\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:660\u001b[0m, in \u001b[0;36mDictReplayBuffer.sample\u001b[1;34m(self, batch_size, env)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    649\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    650\u001b[0m     env: Optional[VecNormalize] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    651\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DictReplayBufferSamples:\n\u001b[0;32m    652\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;124;03m    Sample elements from the replay buffer.\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mReplayBuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Languages\\anaconda3\\envs\\gym310\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:113\u001b[0m, in \u001b[0;36mBaseBuffer.sample\u001b[1;34m(self, batch_size, env)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m:param batch_size: Number of element to sample\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m:param env: associated gym VecEnv\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    to normalize the observations/rewards when sampling\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    112\u001b[0m upper_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos\n\u001b[1;32m--> 113\u001b[0m batch_inds \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper_bound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_samples(batch_inds, env\u001b[38;5;241m=\u001b[39menv)\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:780\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mnumpy\\\\random\\\\_bounded_integers.pyx:2885\u001b[0m, in \u001b[0;36mnumpy.random._bounded_integers._rand_int32\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: high <= 0"
     ]
    }
   ],
   "source": [
    "# import panda_gym\n",
    "# import sys\n",
    "# import gymnasium\n",
    "# sys.modules[\"gym\"] = gymnasium\n",
    "# # from stable_baselines3 import DDPG\n",
    "# from sb3_contrib import TQC\n",
    "# from stable_baselines3.common.logger import configure\n",
    "\n",
    "# # set up logger\n",
    "# tmp_path = \"/tmp/sb3_log/\"\n",
    "# new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "\n",
    "# env = gymnasium.make(\"PandaPickAndPlace-v3\")\n",
    "\n",
    "# model = TQC(policy=\"MultiInputPolicy\", env=env, verbose=1)\n",
    "# model.set_logger(new_logger)\n",
    "# model.train(1000)\n",
    "# model.save(\"ppap\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
